{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following packages will be needed to run this notebook.\n",
    "# Please make sure you have them installed on your device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial logistics to load in data & prepare for later manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates an empty dataframe\n",
    "data = pd.DataFrame(columns=['Publisher', 'Application', 'Version', 'Install Date', 'Last HW Scan', 'OS', 'OS Version', 'Encrypted Workstation Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filepath needs to be where the files are located on your device\n",
    "file_paths = glob.glob('/Users/eric/Dropbox/My Mac (Erics-MacBook-Pro-3.local)/Desktop/t4sg-usda/data/*.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "# This might take some time, but you'll be able to see the filenames as they're being read in\n",
    "for filepath in file_paths:\n",
    "    data = data.append(pd.read_excel(filepath))\n",
    "    print(filepath)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the setting so we could see all the lines of the dashboard\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dashboards that do NOT take into account the version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard by application name and number of installations in the whole dataset\n",
    "full_data = data.groupby(['Publisher', 'Application'])\n",
    "full_data = full_data.size().sort_values( ascending=False).reset_index(name='# of all entries')\n",
    "dashboard = pd.DataFrame(full_data)\n",
    "dashboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a dashboard without versions\n",
    "\n",
    "# Dashboard by application name and number of duplicated installations in the whole dataset\n",
    "duplicates = data.where(data.duplicated(subset=['Application', 'Encrypted Workstation Name', 'Last HW Scan']) == True).dropna(how='all')\n",
    "dupl_dash = duplicates.groupby(['Publisher', 'Application'])\n",
    "dupl_dash = dupl_dash.size().sort_values( ascending=False).reset_index(name='# of duplicate installations')\n",
    "dupl_df = pd.DataFrame(dupl_dash, columns = ['Publisher', 'Application', '# of duplicate installations'])\n",
    "\n",
    "# Dashboard by application name and number of not duplicated installations in the whole dataset\n",
    "data_no_dupl = data.where(data.duplicated(subset=['Application', 'Encrypted Workstation Name', 'Last HW Scan']) != True).dropna(how='all')\n",
    "data_no_dupl = data_no_dupl.groupby(['Publisher', 'Application'])\n",
    "data_no_dupl = data_no_dupl.size().sort_values( ascending=False).reset_index(name='# of unique installations')\n",
    "no_dupl = pd.DataFrame(data_no_dupl, columns = ['Publisher', 'Application', '# of unique installations'])\n",
    "\n",
    "# Putting the sub-dashboards all together\n",
    "dashboard = dashboard.merge(dupl_df, how='left', on=['Publisher', 'Application'])\n",
    "dashboard = dashboard.merge(no_dupl, on=['Publisher', 'Application'])\n",
    "dashboard = dashboard.fillna(value=0)\n",
    "dashboard['# of duplicate installations'] = dashboard['# of duplicate installations'].astype(int)\n",
    "dashboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of distinct applications is:')\n",
    "print(dashboard.shape[0])\n",
    "print('The number of duplicated installations is:')\n",
    "print(dupl_df['# of duplicate installations'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the dashboards into Excel files\n",
    "dupl_df.to_excel('duplicates_03_17.xlsx')\n",
    "dashboard.to_excel('full_dashboard_03_17.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dashboards that DO take into account the version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a version based duplicates dashboard\n",
    "version_data = data.copy()\n",
    "\n",
    "version_data['Version'] = version_data['Version'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Full dashboard for applications with versions and the number of all installations\n",
    "v_full_data = version_data.groupby(['Publisher', 'Application', 'Version'])\n",
    "v_full_data = v_full_data.size().sort_values( ascending=False).reset_index(name='# of all entries')\n",
    "v_dashboard = pd.DataFrame(v_full_data)\n",
    "v_dashboard.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking random applications to make sure it matches with the previous dashboards\n",
    "v_dashboard[v_dashboard['Application'] == 'Cisco AnyConnect Secure Mobility Client']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding duplicates by versions\n",
    "v_duplicates = version_data.where(version_data.duplicated(subset=['Application', 'Version', 'Encrypted Workstation Name', 'Last HW Scan']) == True).dropna(how='all')\n",
    "v_dupl_dash = v_duplicates.groupby(['Publisher', 'Application', 'Version'])\n",
    "v_dupl_dash = v_dupl_dash.size().sort_values( ascending=False).reset_index(name='# of duplicate installations')\n",
    "\n",
    "# Finding unique installations by versions\n",
    "v_no_dupl = version_data.where(version_data.duplicated(subset=['Application', 'Version', 'Encrypted Workstation Name', 'Last HW Scan']) != True).dropna(how='all')\n",
    "no_dupl_by_v = v_no_dupl.groupby(['Publisher', 'Application', 'Version'])\n",
    "no_dupl_by_v = no_dupl_by_v.size().sort_values( ascending=False).reset_index(name='# of unique installations')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_dupl_dash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify that when summing the duplicates up by application it the numbers are similar to the previous dashboards\n",
    "v_dupl_byapp = v_duplicates.groupby(['Publisher', 'Application'])\n",
    "v_dupl_byapp = v_dupl_byapp.size().sort_values( ascending=False).reset_index(name='# of duplicate installations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dupl_byapp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything into the full dashboard\n",
    "v_no_dupl_df = pd.DataFrame(no_dupl_by_v, columns = ['Publisher', 'Application', 'Version', '# of unique installations'])\n",
    "v_dupl_df = pd.DataFrame(v_dupl_dash, columns = ['Publisher', 'Application', 'Version', '# of duplicate installations'])\n",
    "v_full = v_dashboard.merge(v_dupl_df, how='left', on=['Publisher', 'Application', 'Version'])\n",
    "v_full = v_full.merge(v_no_dupl_df, on=['Publisher', 'Application', 'Version'])\n",
    "v_full = v_full.fillna(value=0)\n",
    "v_full['# of duplicate installations'] = v_full['# of duplicate installations'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking that it seems logical\n",
    "v_full[v_full['Application'] == 'Cisco AnyConnect Secure Mobility Client']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_full.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_full.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the new dashboards into Excel files\n",
    "v_full.to_excel('full_dashboard_w_versions_03_26.xlsx')\n",
    "v_dupl_dash.to_excel('duplicates_w_versions_03_26.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dashboard by # of unique installations for the bundling algorithm\n",
    "dashboard = dashboard.sort_values(by=['# of unique installations'], ascending=False).reset_index(drop=True)\n",
    "dashboard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundle-identification code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# filter out commonly used words in application titles\n",
    "common_words = ['Tool', 'Module', 'Update', 'Software', '', 'App', 'Client',\n",
    "                'Tools', 'for', 'and', 'in', 'Client', 'Installer', 'Drive', \n",
    "                'Driver', 'Web', 'Helper', 'Support', 'Center', 'Manager',\n",
    "                'File', 'Reader', 'C', 'Launcher', 'Plugin', 'Service', 'Setup', 'Driver',\n",
    "               'x86', '(x86)', 'x64', '(x64)', 'X86']\n",
    "\n",
    "# filter out numbers and version numbers\n",
    "def is_number(string):\n",
    "    matching = re.match(r'^\\d+(\\.\\d+)*$', string)\n",
    "    if matching:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# cleaning the name of characters such as dash, underscore etc.\n",
    "def clean_name(name):\n",
    "    words = re.split(' |,|_|-', name)\n",
    "    words = list(filter(lambda w: w not in common_words, words))\n",
    "    words = list(filter(lambda w: not is_number(w), words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### create a spreadsheet that lists applications and their potential bundles based on fuzzywuzzy scores\n",
    "\n",
    "# Converts application at row index to form \"[name] || [publisher] || [version]\"\n",
    "def stringify(dashboard, index):\n",
    "    name = dashboard.loc[[index]]['Application'].iat[0]\n",
    "    pub = dashboard.loc[[index]]['Publisher'].iat[0]\n",
    "    version = dashboard.loc[[index]]['Version'].iat[0]\n",
    "    return name + \" || \" + pub + \" || \" + version\n",
    "\n",
    "# Confirms that cleaned potential bundled application names have common words\n",
    "def checkIfSubword(name, other):\n",
    "    name_clean = clean_name(name)\n",
    "    other_clean = clean_name(other)\n",
    "    # If any of the apps are 1 word long, don't have to have a common string\n",
    "    if len(name) == 1 or len(other) == 1:\n",
    "        return True\n",
    "    return bool(set(name_clean) & set(other_clean))\n",
    "\n",
    "# Checks that two applications are installed on at least 70% of the same workstations\n",
    "def checkIfSimilarWorkstations(name, other):\n",
    "    filtered_1 = data[data.Application == name]\n",
    "    filtered_2 = data[data.Application == other]\n",
    "    flist1 = filtered_1[\"Encrypted Workstation Name\"].to_list()\n",
    "    flist2 = filtered_2[\"Encrypted Workstation Name\"].to_list()\n",
    "    res = len(set(flist1) & set(flist2)) / float(len(set(flist1) | set(flist2))) * 100\n",
    "    # This similarity threshold (70%) can be adjusted higher or lower\n",
    "    return res >= 70\n",
    "\n",
    "# Checks that two applications have the same version number\n",
    "def checkVersion(dashboard, index, other_index):\n",
    "    v1 = dashboard.loc[[index]]['Version'].iat[0]\n",
    "    v2 = dashboard.loc[[other_index]]['Version'].iat[0]\n",
    "    return v1 == v2\n",
    "\n",
    "# Checks if two applications should be bundled\n",
    "def checkIfBundle(dashboard, index, other_index):\n",
    "    name = dashboard.loc[[index]]['Application'].iat[0]\n",
    "    other = dashboard.loc[[other_index]]['Application'].iat[0]\n",
    "    name_clean = clean_name(name)\n",
    "    other_clean = clean_name(other)\n",
    "    # Fuzzywuzzy checks that the names of the two applications are roughly similar (>50% similarity)\n",
    "    return checkVersion(dashboard, index, other_index) and checkIfSubword(name, other) and fuzz.partial_ratio(name_clean, other_clean) >= 50 and checkIfSimilarWorkstations(name, other)\n",
    "\n",
    "# Creates a csv of potential bundles, taking in a dashboard as input\n",
    "def createBundle(dashboard):\n",
    "    # Sorts dashboard by number of unique installations\n",
    "    dashboard = dashboard.sort_values(by=['# of unique installations'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    maxRows = len(dashboard.index)\n",
    "    bundleList = []\n",
    "    \n",
    "    # Creates a dashboard column \"grouped\" that indicates whether an application has been bundled\n",
    "    dashboard['grouped'] = False\n",
    "    \n",
    "    # needs to be sorted by count\n",
    "    for index, row in dashboard.iterrows():\n",
    "        bundle = []\n",
    "        count = row['# of unique installations']\n",
    "        # stop algo if # of installations is less than 100\n",
    "        if count < 100:\n",
    "            break\n",
    "        # if the application has been grouped, skip it\n",
    "        if dashboard.loc[[index]]['grouped'].iat[0] == True:\n",
    "            continue\n",
    "        name = row['Application']\n",
    "        # parse through above rows until count >10% difference\n",
    "        tempIndex = index - 1\n",
    "        while tempIndex >= 0 and dashboard.loc[[tempIndex]]['# of unique installations'].iat[0] - count <= count/10:\n",
    "            if checkIfBundle(dashboard, index, tempIndex):\n",
    "                # Setting grouped for the application being analyzed\n",
    "                dashboard.at[index, 'grouped'] = True\n",
    "                dashboard.at[tempIndex, 'grouped'] = True\n",
    "                bundle.append(stringify(dashboard, tempIndex))\n",
    "            tempIndex -= 1\n",
    "        # parse through below rows until count <10% difference\n",
    "        tempIndex = index + 1\n",
    "        while tempIndex < maxRows and count - dashboard.loc[[tempIndex]]['# of unique installations'].iat[0] <= count/10:\n",
    "            if checkIfBundle(dashboard, index, tempIndex):\n",
    "                dashboard.at[index, 'grouped'] = True\n",
    "                dashboard.at[tempIndex, 'grouped'] = True\n",
    "                bundle.append(stringify(dashboard, tempIndex))\n",
    "            tempIndex += 1\n",
    "        # indicate that the program is running\n",
    "        if index % 10 == 0:\n",
    "            print(index, end=' ')\n",
    "        # insert the app name and the count\n",
    "        bundle.insert(0, stringify(dashboard, index))\n",
    "        bundle.insert(1, count)\n",
    "        bundleList.append(bundle)\n",
    "        \n",
    "    # Creates v_bundles.csv with bundles\n",
    "    df = pd.DataFrame(bundleList)\n",
    "    df.to_csv('v_bundles.csv', index=False, header=False)\n",
    "    \n",
    "    # prints Done when algo has finished\n",
    "    print(\"Done\")\n",
    "\n",
    "# Creates a csv of bundles from the v_full dashboard (defined above)\n",
    "createBundle(v_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify problematic applications that have typos in publisher name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_vers = []\n",
    "\n",
    "# Detecting applications that have multiple publishers for the same versions and over 100 unique installations\n",
    "for index, row in v_full[v_full['# of unique installations'] > 100].iterrows():\n",
    "    if len(v_full[(v_full['Application'] == row['Application']) & (v_full['# of unique installations'] > 100) & (v_full['Version'] == row['Version'])]['# of unique installations']) != 1:\n",
    "        if (row['Application'], row['Version']) not in prob_vers:\n",
    "            prob_vers.append((row['Application'], row['Version']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_vers_df = pd.DataFrame(columns=['Application', 'Version', 'Publisher || # of unique installations'])\n",
    "\n",
    "# Creating the table for the problematic applications and their publishers\n",
    "for index, row in v_full[v_full['# of unique installations'] > 100].iterrows():\n",
    "#     Detecting if the application has already been added to the table\n",
    "    if (row['Application'], row['Version']) in prob_vers and row['Application'] not in list(prob_vers_df['Application']):\n",
    "        pubs = v_full[(v_full['Application'] == row['Application']) & (v_full['Version'] == row['Version'])]['Publisher']\n",
    "        counts = v_full[(v_full['Application'] == row['Application']) & (v_full['Version'] == row['Version'])]['# of unique installations']\n",
    "#         separating the publishers with pipes\n",
    "        pub_list = '; '.join([pubs.iloc[i] + ' || ' + str(counts.iloc[i]) for i in range(0, len(pubs))])\n",
    "        prob_vers_df = prob_vers_df.append({'Application': row['Application'], 'Version': row['Version'], 'Publisher || # of unique installations': pub_list}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_vers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_vers_df.to_excel('problematic_apps_04_12.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
